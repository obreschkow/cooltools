% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropy.R
\name{entropy}
\alias{entropy}
\title{Information entropy}
\usage{
entropy(p, b = exp(1), normalize = TRUE)
}
\arguments{
\item{p}{vector of probabilities; typically normalized, such that sum(p)=1.}

\item{b}{base of the logarithm (default is e)}

\item{normalize}{logical flag. If TRUE (default), the vector p is automatically normalized.}
}
\value{
Returns the information entropy in units that depend on b. If b=2, the units are bits; if b=exp(1), the units are nats; if b=10, the units are dits.
}
\description{
Computes the information entropy H=sum(p*log_b(p)), also known as Shannon entropy, of a probability vector p.
}
\author{
Danail Obreschkow
}
